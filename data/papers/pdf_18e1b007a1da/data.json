{
  "paper_id": "pdf_18e1b007a1da",
  "filename": "pdf",
  "storage_path": "data/papers/pdf_18e1b007a1da",
  "title": "Attention Is All You Need",
  "authors": [
    {
      "name": "Ashish Vaswani",
      "affiliation": "Google Brain"
    },
    {
      "name": "Noam Shazeer",
      "affiliation": "Google Brain"
    },
    {
      "name": "Niki Parmar",
      "affiliation": "Google Brain"
    },
    {
      "name": "Jakob Uszkoreit",
      "affiliation": "Google Brain"
    },
    {
      "name": "Llion Jones",
      "affiliation": "Google Brain"
    },
    {
      "name": "Aidan Gomez",
      "affiliation": "Google Brain"
    },
    {
      "name": "Łukasz Kaiser",
      "affiliation": "Google Brain"
    },
    {
      "name": "Illia Polosukhin",
      "affiliation": "Google Brain"
    },
    {
      "name": "Jimmy Lei Ba",
      "affiliation": "Unknown"
    },
    {
      "name": "Jamie Ryan Kiros",
      "affiliation": "Unknown"
    }
  ],
  "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. * Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.",
  "sections": [
    {
      "title": "Introduction",
      "content": "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5]. Numerous efforts have s..."
    },
    {
      "title": "Background",
      "content": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these model..."
    },
    {
      "title": "Model Architecture",
      "content": "Most competitive neural sequence transduction models have an encoder-decoder structure [5,2,35]. Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output seque..."
    },
    {
      "title": "Encoder and Decoder Stacks",
      "content": "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-lay..."
    },
    {
      "title": "Attention",
      "content": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum Scaled Dot-Product Attention Multi-Head Attention of the values, where the weight assigned to each ..."
    },
    {
      "title": "Scaled Dot-Product Attention",
      "content": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the..."
    },
    {
      "title": "Multi-Head Attention",
      "content": "Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected v..."
    },
    {
      "title": "Applications of Attention in our Model",
      "content": "The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in ..."
    },
    {
      "title": "Position-wise Feed-Forward Networks",
      "content": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. While the linear transforma..."
    },
    {
      "title": "Embeddings and Softmax",
      "content": "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In ..."
    },
    {
      "title": "Positional Encoding",
      "content": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the..."
    },
    {
      "title": "Why Self-Attention",
      "content": "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x 1 , ..., x n ) to another sequence of equal length (z 1 , ..., z n ), with x i , z i ∈ R d , such as a hid..."
    },
    {
      "title": "Training",
      "content": "This section describes the training regime for our models."
    },
    {
      "title": "Training Data and Batching",
      "content": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-Fren..."
    }
  ],
  "references": [
    "Vaswani, Shazeer. \"Attention Is All You Need\"",
    "Lei Ba, Ryan Kiros. \"Layer normalization\"",
    "Bahdanau, Cho. \"Neural machine translation by jointly learning to align and translate\"",
    "Britz, Goldie. \"Massive exploration of neural machine translation architectures\"",
    "Cheng, Dong. \"Long short-term memory-networks for machine reading\"",
    "Cho, Van Merrienboer. \"Learning phrase representations using rnn encoder-decoder for statistical mac...",
    "Chollet. \"Xception: Deep learning with depthwise separable convolutions\"",
    "Chung, Gülçehre. \"Empirical evaluation of gated recurrent neural networks on sequence modeling\"",
    "Dyer, Kuncoro. \"Recurrent neural network grammars\"",
    "Gehring, Auli. \"Convolutional sequence to sequence learning\""
  ],
  "keywords": [],
  "figures": [
    {
      "figure_id": "page_3_img_1",
      "page": 3,
      "size": [
        1520,
        2239
      ],
      "file_path": "data/papers/pdf_18e1b007a1da/figures/page_3_img_1.png"
    },
    {
      "figure_id": "page_4_img_1",
      "page": 4,
      "size": [
        445,
        884
      ],
      "file_path": "data/papers/pdf_18e1b007a1da/figures/page_4_img_1.png"
    },
    {
      "figure_id": "page_4_img_2",
      "page": 4,
      "size": [
        835,
        1282
      ],
      "file_path": "data/papers/pdf_18e1b007a1da/figures/page_4_img_2.png"
    }
  ],
  "total_figures": 3,
  "processing_time": 16.40357542037964,
  "status": "success"
}